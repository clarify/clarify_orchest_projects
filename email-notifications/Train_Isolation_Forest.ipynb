{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87cde983-13ac-4c57-bce5-86d80701fa32",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Anomaly Detection\n",
    "# Training stage\n",
    "\n",
    "The goal here is to train an anomaly detection algorithm in order to find anomalies in our data. As we will see later, once we have trained the model, we can save it and use it on new (streaming) data points. Once new anomalies are found we will get an email notification about them and write them back to Clarify in a new signal.\n",
    "\n",
    "# Isolation Forest \n",
    "To perform anomaly detection, we will use Isolation Forest (or iForest). As mentioned in the [Isolation Forest paper](https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf?q=isolation-forest) anomalies are here considered to be those which are 'few and different'. These data points can be isolated more easily than 'normal' data points. Therefore anomalies are isolated closer to the root of the tree, also known as Isolation Tree (or iTree) \n",
    "\n",
    "<a href=\"https://www.researchgate.net/figure/Isolation-Forest-learned-iForest-construction-for-toy-dataset_fig1_352017898\"><img src=\"https://www.researchgate.net/publication/352017898/figure/fig1/AS:1029757483372550@1622524724599/Isolation-Forest-learned-iForest-construction-for-toy-dataset.png\" alt=\"Isolation Forest: learned iForest construction for toy dataset\"/></a>\n",
    "\n",
    "[Source](https://www.researchgate.net/figure/Isolation-Forest-learned-iForest-construction-for-toy-dataset_fig1_352017898) Scientific Figure on ResearchGate \n",
    "\n",
    "\n",
    "If we create multiple trees we can get the average path lengths from all the data points and from that optain the anomaly score values. The top `m` data points with the corresponding lowest anomaly score values are considered as anomalies.\n",
    "\n",
    "In order to gain a better understaning of the algorithm's performance, in the following figures we will plot the predict values (which are either 1 or -1) and the average anomaly scores (negative scores represent outliers and positive scores represent normal points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12458a0-b714-4458-b704-e0f380423e7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import orchest\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "# Get data from previous step\n",
    "orchest_data = orchest.get_inputs()\n",
    "response = orchest_data[\"response\"]\n",
    "outliers_fraction = orchest.get_step_param(\"outliers_fraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c5773a-22cc-41fa-9a15-895c6bde9336",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dates = response.index\n",
    "values = sum(response.values.tolist(), [])\n",
    "df = pd.DataFrame({'date': dates, 'x': values})\n",
    "\n",
    "X = df[[\"x\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dbc227-02e8-423c-ae88-f3c98398f361",
   "metadata": {},
   "source": [
    "In sklearn the number of trees which will be created are by default 100, and the sub-sampling size is by default 256. \n",
    "We will change these values to reduce the run time and the swampling and masking effects on our data points. \n",
    "\n",
    "**Swampling** is finding false anomalies. When anomaly points are close to normal points the process of finding these anomaly points is harder and can easiely lead to categorizing normal points as anomalies. \n",
    "\n",
    "**Masking** is called the phenomenon where we have many anomalies building a small cluster, therefore ending up to be categorized as normal points. \n",
    "\n",
    "\n",
    "These two phenomenons can be tuned by the sub-samplint size parameter (called max_samples in sklearn). Because we don't need to isolate all data points, if we reduce the sub-samlping size, we can obtain more true anomaly points. \n",
    "Note that we use sub-sampling without replacement.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/clarify/data-science-tutorials/main/media/email_notifications/isolationg.png\"/>\n",
    "\n",
    "[Source Isolation Forest paper](https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf?q=isolation-forest) \n",
    "(a) Isolating a normal point Xi needs more splitting procedures than (b) isolating an anomaly point Xo.\n",
    "\n",
    "\n",
    "The complexity of the training process can be tuned with the number of trees used (called n_estimators in sklearn). As mentioned in the  [Isolation Forest paper](https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf?q=isolation-forest) for n_estimators = 100 the path lengths usually converge well. \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/clarify/data-science-tutorials/main/media/email_notifications/before.png\" width=312 height=312 />  <img src=\"https://raw.githubusercontent.com/clarify/data-science-tutorials/main/media/email_notifications/after.png\" width=300 height=300 />\n",
    "\n",
    "[Source Isolation Forest paper](https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf?q=isolation-forest) \n",
    "The first plot has 4096 instances, the second plot has 128 instanses\n",
    "\n",
    "\n",
    "Last but not least, when using Isolation Forest on time series data it usually helps to set the contamination parameter. This parameter defines the threshold on the scores when fitting the data on the model.\n",
    "The default value is auto, which will set the threshold as determined in the original paper. When using contamination = 'auto' the anomaly points which the algorithm finds are way too many as we can see in the plot below.\n",
    "In this notebook we will use a value of 0.01 for contamination which represents the proportion of outliers in the data set. \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/clarify/data-science-tutorials/main/media/email_notifications/anomalies.png\"/>\n",
    "Anomaly points when setting contamination to 'auto'. Use contamination = 0.01 to get better results.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dad99c8-63cc-4b48-a2ce-12ee238d70db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set models parameters\n",
    "print(\"Outliers fraction is set to: \", outliers_fraction)\n",
    "rng = np.random.RandomState(100)\n",
    "model = IsolationForest(n_estimators = 90, max_samples=200, contamination=outliers_fraction, random_state=rng)\n",
    "model.fit(X)\n",
    "pred = model.predict(X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a28deeb-c579-4541-bbc1-3bf3c6881bb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fast hack to see the score regions. \n",
    "\n",
    "X_ = np.column_stack((np.array(df[\"x\"]), np.arange(0, len(df[\"x\"]))))\n",
    "rng = np.random.RandomState(100)\n",
    "Xmodel = IsolationForest(n_estimators = 90, max_samples=200, contamination=outliers_fraction, random_state=rng)\n",
    "Xmodel.fit(X_)\n",
    "\n",
    "rcParams['figure.figsize'] = 30, 13\n",
    "l = len(df[\"x\"])\n",
    "xx, yy = np.meshgrid(np.linspace(0, l, 50), np.linspace(0, l, 50))\n",
    "Z = Xmodel.decision_function(np.c_[yy.ravel(), xx.ravel()]) # decision_function returns the anomaly scores\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.title(\"IsolationForest\")\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.Blues_r)\n",
    "\n",
    "b1 = plt.scatter(X_[:, 1], X_[:, 0], c=\"yellow\", s=40, edgecolor=\"k\")\n",
    "plt.axis(\"tight\")\n",
    "plt.xlim((0, l))\n",
    "plt.ylim((0, l))\n",
    "plt.legend([b1],[\"training observations\"],loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94998ed-6c39-4726-a956-3117d616e444",
   "metadata": {
    "tags": []
   },
   "source": [
    "In the plot above we can see an example of different colors of regions. \n",
    "\n",
    "The darker the color is, the more likely is that the points in this area are anomaly points. \n",
    "\n",
    "These regions where created by using the anomaly scores for x = (0,1, ... 50) and y = (0,1,...50) on the 2d fitted model. \n",
    "\n",
    "This way we can have the scores for the hole grid.\n",
    "\n",
    "Note that we will not use the 2d model since we have only one time series. This plot was created only as an example - as the results of the 1d and 2d model are fairly similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc56f79-63f5-4cda-a5b5-c92701d9f136",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scores = model.decision_function(X)\n",
    "l = len(scores)\n",
    "\n",
    "fig = go.Figure(data=go.Scatter(x = np.arange(0,l), y = scores))\n",
    "fig.update_layout(title = \"Score values\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90b2270-5f59-48aa-b04a-1826f2737c2e",
   "metadata": {},
   "source": [
    "We can also plot the score values from the training set X. Negative scores represent outliers, positive scores represent inliers. If we zoom in the plot, we can easily find which data points are found as anomalies. Note that many lines are very close to y = 0. Therefore the algorithm is not sure if these points are anomalies or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56229473-d47e-406b-9b26-851d829f1c0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['anomaly_score'] = pd.Series(pred)\n",
    "anomaly = df.loc[df['anomaly_score'] == -1, ['date', 'x']] \n",
    "\n",
    "print(\"anomalies found:\")\n",
    "anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4010fb-458d-4990-af1e-ccd4f8940690",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = go.Figure(data=go.Scatter(x = np.arange(0,len(pred)), y = pred))\n",
    "fig.update_layout(title = \"predict values\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ee7250-7916-4735-8b69-8f8067485559",
   "metadata": {},
   "source": [
    "The `predict` method returns the values 1 or -1. With -1 are marked the data points which corresponde to a negavive score value and with 1 are marked the data points which corresponde to a positive score value.\n",
    "\n",
    "In the above plot we can see that the same data points which have a negative score value, have a -1 predict value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d971255-4c65-4ccf-af51-276f7dee23f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = go.Figure(data=go.Scatter(x = df['date'], y = df['x'], mode='markers', name = \"Normal values\", marker=dict(color='blue', size=4)))\n",
    "fig.add_traces(go.Scatter(x = anomaly['date'], y = anomaly['x'], textposition='top left',\n",
    "                          textfont=dict(color='#233a77'),\n",
    "                          mode='markers+text',\n",
    "                          name = \"Anomaly value\",\n",
    "                          marker=dict(color='red', size=6)))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9b5b32-bf87-4831-bfde-956de22b32e2",
   "metadata": {},
   "source": [
    "Last but not least, we plot the 'normal' data points with blue, and the anomaly data points with red.\n",
    "\n",
    "And save the model so that we can use it on new (streaming) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41505222-d844-45e6-b22d-8606ff2b9fb0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-07T13:24:29.369643Z",
     "iopub.status.busy": "2022-04-07T13:24:29.368993Z",
     "iopub.status.idle": "2022-04-07T13:24:29.379021Z",
     "shell.execute_reply": "2022-04-07T13:24:29.378201Z"
    }
   },
   "outputs": [],
   "source": [
    "file = '../data/model.sav'\n",
    "f = open(file,'x')\n",
    "f.close()\n",
    "\n",
    "with open(file, 'wb') as f:\n",
    "    print(\"Save model in model file...\")\n",
    "    pickle.dump(model, f)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77849183-656b-4bee-821f-00712bd50802",
   "metadata": {},
   "source": [
    "As an end note, some of you might be wondering why we didn't mentioned anything about normalizing our data. \n",
    "This is a good observation. The first question that we should ask is if it is needed. Since we are using an anomaly detection algorithm, normalizing our data would maybe make it harder to the algorithm to find anomaly points. By scaling our data, it will probably not play a big role, since the way the Isolation Forest algorithm works is by randomly selecting a value between the min and max value and spit them. What the min and max values are doesn't matter, but the bigger the range is, the easier it probably is to find anomaly points. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "orchest-kernel-69a43e05-ec5e-4c78-884e-d753d093ab81"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
